{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3302b95a",
   "metadata": {},
   "source": [
    "## Import Requird Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de39da95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    MT5ForConditionalGeneration,\n",
    "    ByT5Tokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    T5TokenizerFast as T5Tokenizer,\n",
    "    MT5TokenizerFast as MT5Tokenizer,\n",
    "    T5Model,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer\n",
    ")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d6d7f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import evaluate  # Bleu\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import T5Tokenizer, T5Model, T5ForConditionalGeneration, T5TokenizerFast\n",
    "\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bd2ff2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.7.4-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.3.1 in /opt/finicity/spandey/t5_model/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.4.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.47.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/finicity/spandey/t5_model/lib/python3.8/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/finicity/spandey/t5_model/lib/python3.8/site-packages (from matplotlib) (5.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/finicity/spandey/t5_model/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading pillow-10.2.0-cp38-cp38-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1.20 in /opt/finicity/spandey/t5_model/lib/python3.8/site-packages (from matplotlib) (1.23.4)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/finicity/spandey/t5_model/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/finicity/spandey/t5_model/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.1.1 cycler-0.12.1 fonttools-4.47.2 kiwisolver-1.4.5 matplotlib-3.7.4 pillow-10.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda3f6f6",
   "metadata": {},
   "source": [
    "##  Load  Flan-T5-Small Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "521d7216",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_model=AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\", return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43e4f13",
   "metadata": {},
   "source": [
    "##  Load  Flan-T5-Small Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff68ffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c11ec9",
   "metadata": {},
   "source": [
    "## Article Sumarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f12008c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'APJ abdul Kalam was born in a poor Tamil Muslim family on 15th of October in 1931 at Rameshwaram, Ramnad district of Madras presidency under British India'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", model=t5_model,tokenizer=tokenizer)\n",
    "summarizer(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fafa0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> APJ abdul Kalam was born in a poor Tamil Muslim family on 15th\n"
     ]
    }
   ],
   "source": [
    "input_text = \"summarize: The full name of Dr. APJ abdul Kalam was Avul Pakir Jainulabdeen Abdul Kalam. He is popularly known as the Missile Man of India and People’s President. Kalam was born in a poor Tamil Muslim family on 15th of October in 1931 at Rameshwaram, Ramnad district of Madras presidency under British India (currently in Ramanathapuram District, Tamil Nadu). He was a great scientist who also served the country as the 11th President of India from 2002 to 2007. After completing his term of presidency, he returned to the civilian life of writing, education, and public service. He worked at various chief positions at ISRO and DRDO then became a Principal Scientific Adviser to the Government Of India as a Cabinet Minister.Kalam has been honored with the honorary doctorates by at least 30 universities as well as three highest civilian awards of the country (Padma Bhushan 1981, Padma Vibhushan 1990 and Bharat Ratna 1997). He was a great personality and inspiration to the youngsters of country who took his last breath at IIM, Meghalaya on 27th of July in 2015 because of the sudden cardiac arrest. Kalam is not present among us physically however his great works and contributions would be with us forever. He has mentioned his dream of making India a developed country in his book “India 2020-A vision for the New Millennium. \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = t5_model.generate(input_ids)\n",
    "for i in outputs:\n",
    "    print(tokenizer.decode(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec92070b",
   "metadata": {},
   "source": [
    "## Question Answering  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17f38235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> one and one is the sum of two and one, one and two, and one and a half, two and three, three and one</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "input_text=\"question-answering: What is sum of one and one ?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = t5_model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93864b1",
   "metadata": {},
   "source": [
    "### Translation Task From English to French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23027114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> M'écran ne s'agit pas de l'écran. et il n'y a pas d'un écran.</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "input_text=\"english to french: My screen is blank ?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = t5_model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e7f9de",
   "metadata": {},
   "source": [
    "### Layer Name and Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "24840530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared.weight : Shape 32128\n",
      "encoder.block.0.layer.0.SelfAttention.q.weight : Shape 384\n",
      "encoder.block.0.layer.0.SelfAttention.k.weight : Shape 384\n",
      "encoder.block.0.layer.0.SelfAttention.v.weight : Shape 384\n",
      "encoder.block.0.layer.0.SelfAttention.o.weight : Shape 512\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight : Shape 32\n",
      "encoder.block.0.layer.0.layer_norm.weight : Shape 512\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_0.weight : Shape 1024\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_1.weight : Shape 1024\n",
      "encoder.block.0.layer.1.DenseReluDense.wo.weight : Shape 512\n",
      "encoder.block.0.layer.1.layer_norm.weight : Shape 512\n",
      "encoder.block.1.layer.0.SelfAttention.q.weight : Shape 384\n",
      "encoder.block.1.layer.0.SelfAttention.k.weight : Shape 384\n",
      "encoder.block.1.layer.0.SelfAttention.v.weight : Shape 384\n",
      "encoder.block.1.layer.0.SelfAttention.o.weight : Shape 512\n",
      "encoder.block.1.layer.0.layer_norm.weight : Shape 512\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_0.weight : Shape 1024\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_1.weight : Shape 1024\n",
      "encoder.block.1.layer.1.DenseReluDense.wo.weight : Shape 512\n",
      "encoder.block.1.layer.1.layer_norm.weight : Shape 512\n",
      "encoder.block.2.layer.0.SelfAttention.q.weight : Shape 384\n",
      "encoder.block.2.layer.0.SelfAttention.k.weight : Shape 384\n",
      "encoder.block.2.layer.0.SelfAttention.v.weight : Shape 384\n",
      "encoder.block.2.layer.0.SelfAttention.o.weight : Shape 512\n",
      "encoder.block.2.layer.0.layer_norm.weight : Shape 512\n",
      "encoder.block.2.layer.1.DenseReluDense.wi_0.weight : Shape 1024\n",
      "encoder.block.2.layer.1.DenseReluDense.wi_1.weight : Shape 1024\n",
      "encoder.block.2.layer.1.DenseReluDense.wo.weight : Shape 512\n",
      "encoder.block.2.layer.1.layer_norm.weight : Shape 512\n",
      "encoder.block.3.layer.0.SelfAttention.q.weight : Shape 384\n",
      "encoder.block.3.layer.0.SelfAttention.k.weight : Shape 384\n",
      "encoder.block.3.layer.0.SelfAttention.v.weight : Shape 384\n",
      "encoder.block.3.layer.0.SelfAttention.o.weight : Shape 512\n",
      "encoder.block.3.layer.0.layer_norm.weight : Shape 512\n",
      "encoder.block.3.layer.1.DenseReluDense.wi_0.weight : Shape 1024\n",
      "encoder.block.3.layer.1.DenseReluDense.wi_1.weight : Shape 1024\n",
      "encoder.block.3.layer.1.DenseReluDense.wo.weight : Shape 512\n",
      "encoder.block.3.layer.1.layer_norm.weight : Shape 512\n",
      "encoder.block.4.layer.0.SelfAttention.q.weight : Shape 384\n",
      "encoder.block.4.layer.0.SelfAttention.k.weight : Shape 384\n",
      "encoder.block.4.layer.0.SelfAttention.v.weight : Shape 384\n",
      "encoder.block.4.layer.0.SelfAttention.o.weight : Shape 512\n",
      "encoder.block.4.layer.0.layer_norm.weight : Shape 512\n",
      "encoder.block.4.layer.1.DenseReluDense.wi_0.weight : Shape 1024\n",
      "encoder.block.4.layer.1.DenseReluDense.wi_1.weight : Shape 1024\n",
      "encoder.block.4.layer.1.DenseReluDense.wo.weight : Shape 512\n",
      "encoder.block.4.layer.1.layer_norm.weight : Shape 512\n",
      "encoder.block.5.layer.0.SelfAttention.q.weight : Shape 384\n",
      "encoder.block.5.layer.0.SelfAttention.k.weight : Shape 384\n",
      "encoder.block.5.layer.0.SelfAttention.v.weight : Shape 384\n",
      "encoder.block.5.layer.0.SelfAttention.o.weight : Shape 512\n",
      "encoder.block.5.layer.0.layer_norm.weight : Shape 512\n",
      "encoder.block.5.layer.1.DenseReluDense.wi_0.weight : Shape 1024\n",
      "encoder.block.5.layer.1.DenseReluDense.wi_1.weight : Shape 1024\n",
      "encoder.block.5.layer.1.DenseReluDense.wo.weight : Shape 512\n",
      "encoder.block.5.layer.1.layer_norm.weight : Shape 512\n",
      "encoder.block.6.layer.0.SelfAttention.q.weight : Shape 384\n",
      "encoder.block.6.layer.0.SelfAttention.k.weight : Shape 384\n",
      "encoder.block.6.layer.0.SelfAttention.v.weight : Shape 384\n",
      "encoder.block.6.layer.0.SelfAttention.o.weight : Shape 512\n",
      "encoder.block.6.layer.0.layer_norm.weight : Shape 512\n",
      "encoder.block.6.layer.1.DenseReluDense.wi_0.weight : Shape 1024\n",
      "encoder.block.6.layer.1.DenseReluDense.wi_1.weight : Shape 1024\n",
      "encoder.block.6.layer.1.DenseReluDense.wo.weight : Shape 512\n",
      "encoder.block.6.layer.1.layer_norm.weight : Shape 512\n",
      "encoder.block.7.layer.0.SelfAttention.q.weight : Shape 384\n",
      "encoder.block.7.layer.0.SelfAttention.k.weight : Shape 384\n",
      "encoder.block.7.layer.0.SelfAttention.v.weight : Shape 384\n",
      "encoder.block.7.layer.0.SelfAttention.o.weight : Shape 512\n",
      "encoder.block.7.layer.0.layer_norm.weight : Shape 512\n",
      "encoder.block.7.layer.1.DenseReluDense.wi_0.weight : Shape 1024\n",
      "encoder.block.7.layer.1.DenseReluDense.wi_1.weight : Shape 1024\n",
      "encoder.block.7.layer.1.DenseReluDense.wo.weight : Shape 512\n",
      "encoder.block.7.layer.1.layer_norm.weight : Shape 512\n",
      "encoder.final_layer_norm.weight : Shape 512\n",
      "decoder.block.0.layer.0.SelfAttention.q.weight : Shape 384\n",
      "decoder.block.0.layer.0.SelfAttention.k.weight : Shape 384\n",
      "decoder.block.0.layer.0.SelfAttention.v.weight : Shape 384\n",
      "decoder.block.0.layer.0.SelfAttention.o.weight : Shape 512\n",
      "decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight : Shape 32\n",
      "decoder.block.0.layer.0.layer_norm.weight : Shape 512\n",
      "decoder.block.0.layer.1.EncDecAttention.q.weight : Shape 384\n",
      "decoder.block.0.layer.1.EncDecAttention.k.weight : Shape 384\n",
      "decoder.block.0.layer.1.EncDecAttention.v.weight : Shape 384\n",
      "decoder.block.0.layer.1.EncDecAttention.o.weight : Shape 512\n",
      "decoder.block.0.layer.1.layer_norm.weight : Shape 512\n",
      "decoder.block.0.layer.2.DenseReluDense.wi_0.weight : Shape 1024\n",
      "decoder.block.0.layer.2.DenseReluDense.wi_1.weight : Shape 1024\n",
      "decoder.block.0.layer.2.DenseReluDense.wo.weight : Shape 512\n",
      "decoder.block.0.layer.2.layer_norm.weight : Shape 512\n",
      "decoder.block.1.layer.0.SelfAttention.q.weight : Shape 384\n",
      "decoder.block.1.layer.0.SelfAttention.k.weight : Shape 384\n",
      "decoder.block.1.layer.0.SelfAttention.v.weight : Shape 384\n",
      "decoder.block.1.layer.0.SelfAttention.o.weight : Shape 512\n",
      "decoder.block.1.layer.0.layer_norm.weight : Shape 512\n",
      "decoder.block.1.layer.1.EncDecAttention.q.weight : Shape 384\n",
      "decoder.block.1.layer.1.EncDecAttention.k.weight : Shape 384\n",
      "decoder.block.1.layer.1.EncDecAttention.v.weight : Shape 384\n",
      "decoder.block.1.layer.1.EncDecAttention.o.weight : Shape 512\n",
      "decoder.block.1.layer.1.layer_norm.weight : Shape 512\n",
      "decoder.block.1.layer.2.DenseReluDense.wi_0.weight : Shape 1024\n",
      "decoder.block.1.layer.2.DenseReluDense.wi_1.weight : Shape 1024\n",
      "decoder.block.1.layer.2.DenseReluDense.wo.weight : Shape 512\n",
      "decoder.block.1.layer.2.layer_norm.weight : Shape 512\n",
      "decoder.block.2.layer.0.SelfAttention.q.weight : Shape 384\n",
      "decoder.block.2.layer.0.SelfAttention.k.weight : Shape 384\n",
      "decoder.block.2.layer.0.SelfAttention.v.weight : Shape 384\n",
      "decoder.block.2.layer.0.SelfAttention.o.weight : Shape 512\n",
      "decoder.block.2.layer.0.layer_norm.weight : Shape 512\n",
      "decoder.block.2.layer.1.EncDecAttention.q.weight : Shape 384\n",
      "decoder.block.2.layer.1.EncDecAttention.k.weight : Shape 384\n",
      "decoder.block.2.layer.1.EncDecAttention.v.weight : Shape 384\n",
      "decoder.block.2.layer.1.EncDecAttention.o.weight : Shape 512\n",
      "decoder.block.2.layer.1.layer_norm.weight : Shape 512\n",
      "decoder.block.2.layer.2.DenseReluDense.wi_0.weight : Shape 1024\n",
      "decoder.block.2.layer.2.DenseReluDense.wi_1.weight : Shape 1024\n",
      "decoder.block.2.layer.2.DenseReluDense.wo.weight : Shape 512\n",
      "decoder.block.2.layer.2.layer_norm.weight : Shape 512\n",
      "decoder.block.3.layer.0.SelfAttention.q.weight : Shape 384\n",
      "decoder.block.3.layer.0.SelfAttention.k.weight : Shape 384\n",
      "decoder.block.3.layer.0.SelfAttention.v.weight : Shape 384\n",
      "decoder.block.3.layer.0.SelfAttention.o.weight : Shape 512\n",
      "decoder.block.3.layer.0.layer_norm.weight : Shape 512\n",
      "decoder.block.3.layer.1.EncDecAttention.q.weight : Shape 384\n",
      "decoder.block.3.layer.1.EncDecAttention.k.weight : Shape 384\n",
      "decoder.block.3.layer.1.EncDecAttention.v.weight : Shape 384\n",
      "decoder.block.3.layer.1.EncDecAttention.o.weight : Shape 512\n",
      "decoder.block.3.layer.1.layer_norm.weight : Shape 512\n",
      "decoder.block.3.layer.2.DenseReluDense.wi_0.weight : Shape 1024\n",
      "decoder.block.3.layer.2.DenseReluDense.wi_1.weight : Shape 1024\n",
      "decoder.block.3.layer.2.DenseReluDense.wo.weight : Shape 512\n",
      "decoder.block.3.layer.2.layer_norm.weight : Shape 512\n",
      "decoder.block.4.layer.0.SelfAttention.q.weight : Shape 384\n",
      "decoder.block.4.layer.0.SelfAttention.k.weight : Shape 384\n",
      "decoder.block.4.layer.0.SelfAttention.v.weight : Shape 384\n",
      "decoder.block.4.layer.0.SelfAttention.o.weight : Shape 512\n",
      "decoder.block.4.layer.0.layer_norm.weight : Shape 512\n",
      "decoder.block.4.layer.1.EncDecAttention.q.weight : Shape 384\n",
      "decoder.block.4.layer.1.EncDecAttention.k.weight : Shape 384\n",
      "decoder.block.4.layer.1.EncDecAttention.v.weight : Shape 384\n",
      "decoder.block.4.layer.1.EncDecAttention.o.weight : Shape 512\n",
      "decoder.block.4.layer.1.layer_norm.weight : Shape 512\n",
      "decoder.block.4.layer.2.DenseReluDense.wi_0.weight : Shape 1024\n",
      "decoder.block.4.layer.2.DenseReluDense.wi_1.weight : Shape 1024\n",
      "decoder.block.4.layer.2.DenseReluDense.wo.weight : Shape 512\n",
      "decoder.block.4.layer.2.layer_norm.weight : Shape 512\n",
      "decoder.block.5.layer.0.SelfAttention.q.weight : Shape 384\n",
      "decoder.block.5.layer.0.SelfAttention.k.weight : Shape 384\n",
      "decoder.block.5.layer.0.SelfAttention.v.weight : Shape 384\n",
      "decoder.block.5.layer.0.SelfAttention.o.weight : Shape 512\n",
      "decoder.block.5.layer.0.layer_norm.weight : Shape 512\n",
      "decoder.block.5.layer.1.EncDecAttention.q.weight : Shape 384\n",
      "decoder.block.5.layer.1.EncDecAttention.k.weight : Shape 384\n",
      "decoder.block.5.layer.1.EncDecAttention.v.weight : Shape 384\n",
      "decoder.block.5.layer.1.EncDecAttention.o.weight : Shape 512\n",
      "decoder.block.5.layer.1.layer_norm.weight : Shape 512\n",
      "decoder.block.5.layer.2.DenseReluDense.wi_0.weight : Shape 1024\n",
      "decoder.block.5.layer.2.DenseReluDense.wi_1.weight : Shape 1024\n",
      "decoder.block.5.layer.2.DenseReluDense.wo.weight : Shape 512\n",
      "decoder.block.5.layer.2.layer_norm.weight : Shape 512\n",
      "decoder.block.6.layer.0.SelfAttention.q.weight : Shape 384\n",
      "decoder.block.6.layer.0.SelfAttention.k.weight : Shape 384\n",
      "decoder.block.6.layer.0.SelfAttention.v.weight : Shape 384\n",
      "decoder.block.6.layer.0.SelfAttention.o.weight : Shape 512\n",
      "decoder.block.6.layer.0.layer_norm.weight : Shape 512\n",
      "decoder.block.6.layer.1.EncDecAttention.q.weight : Shape 384\n",
      "decoder.block.6.layer.1.EncDecAttention.k.weight : Shape 384\n",
      "decoder.block.6.layer.1.EncDecAttention.v.weight : Shape 384\n",
      "decoder.block.6.layer.1.EncDecAttention.o.weight : Shape 512\n",
      "decoder.block.6.layer.1.layer_norm.weight : Shape 512\n",
      "decoder.block.6.layer.2.DenseReluDense.wi_0.weight : Shape 1024\n",
      "decoder.block.6.layer.2.DenseReluDense.wi_1.weight : Shape 1024\n",
      "decoder.block.6.layer.2.DenseReluDense.wo.weight : Shape 512\n",
      "decoder.block.6.layer.2.layer_norm.weight : Shape 512\n",
      "decoder.block.7.layer.0.SelfAttention.q.weight : Shape 384\n",
      "decoder.block.7.layer.0.SelfAttention.k.weight : Shape 384\n",
      "decoder.block.7.layer.0.SelfAttention.v.weight : Shape 384\n",
      "decoder.block.7.layer.0.SelfAttention.o.weight : Shape 512\n",
      "decoder.block.7.layer.0.layer_norm.weight : Shape 512\n",
      "decoder.block.7.layer.1.EncDecAttention.q.weight : Shape 384\n",
      "decoder.block.7.layer.1.EncDecAttention.k.weight : Shape 384\n",
      "decoder.block.7.layer.1.EncDecAttention.v.weight : Shape 384\n",
      "decoder.block.7.layer.1.EncDecAttention.o.weight : Shape 512\n",
      "decoder.block.7.layer.1.layer_norm.weight : Shape 512\n",
      "decoder.block.7.layer.2.DenseReluDense.wi_0.weight : Shape 1024\n",
      "decoder.block.7.layer.2.DenseReluDense.wi_1.weight : Shape 1024\n",
      "decoder.block.7.layer.2.DenseReluDense.wo.weight : Shape 512\n",
      "decoder.block.7.layer.2.layer_norm.weight : Shape 512\n",
      "decoder.final_layer_norm.weight : Shape 512\n",
      "lm_head.weight : Shape 32128\n"
     ]
    }
   ],
   "source": [
    "for name, param in t5_model.named_parameters():\n",
    "    print(name, \": Shape\" + \" \"+str(len(param) ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ca0955",
   "metadata": {},
   "source": [
    "### Total Number of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "96a9dbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76961152"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in t5_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4391929c",
   "metadata": {},
   "source": [
    "### Class for assigning final decoder layer to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "648d5f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "class T5Model_Ov(nn.Module):\n",
    "    _keys_to_ignore_on_load_unexpected = [\n",
    "        \"decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight\",\n",
    "    ]\n",
    "    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n",
    "\n",
    "    def __init__(self):#, config: t5_model.config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared = nn.Embedding(t5_model_seq.config.vocab_size, t5_model_seq.config.d_model)\n",
    "        self.base_model = T5Model.from_pretrained(\"google/flan-t5-small\")\n",
    "        encoder_config = copy.deepcopy(t5_model_seq.config)\n",
    "        self.encoder =self.base_model.encoder# self.base_model(encoder_config, self.shared)\n",
    "        self.encoder.is_decoder = False\n",
    "        self.encoder.use_cache = False\n",
    "        self.encoder.is_encoder_decoder = False\n",
    "            \n",
    "        #decoder_config = copy.deepcopy(config)\n",
    "        \n",
    "        self.decoder = self.base_model.decoder#self.base_model(decoder_config, self.shared)\n",
    "        \n",
    "        self.decoder.num_layers = self.decoder.config.num_decoder_layers\n",
    "        \n",
    "        self.decoder.is_decoder = True\n",
    "        self.decoder.is_encoder_decoder = False\n",
    "           \n",
    "        # Initialize weights and apply final processing\n",
    "        #self.post_init()\n",
    "\n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.decoder\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "      \n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "#    @replace_return_docstrings(output_type=Seq2SeqModelOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(self ):# -> Union[Tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n",
    "       \n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n",
    "        if head_mask is not None and decoder_head_mask is None:\n",
    "            if self.config.num_layers == self.config.num_decoder_layers:\n",
    "                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)\n",
    "                decoder_head_mask = head_mask\n",
    "\n",
    "        # Encode if needed (training, first prediction pass)\n",
    "        if encoder_outputs is None:\n",
    "            encoder_outputs = self.encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                head_mask=head_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
    "            encoder_outputs = BaseModelOutput(\n",
    "                last_hidden_state=encoder_outputs[0],\n",
    "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
    "            )\n",
    "\n",
    "        hidden_states = encoder_outputs[0]\n",
    "\n",
    "        # Set device for model parallelism\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.decoder.first_device)\n",
    "            hidden_states = hidden_states.to(self.decoder.first_device)\n",
    "            if decoder_input_ids is not None:\n",
    "                decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(self.decoder.first_device)\n",
    "            if decoder_attention_mask is not None:\n",
    "                decoder_attention_mask = decoder_attention_mask.to(self.decoder.first_device)\n",
    "\n",
    "        # Decode\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            inputs_embeds=decoder_inputs_embeds,\n",
    "            past_key_values=past_key_values,\n",
    "            encoder_hidden_states=hidden_states,\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        if not return_dict:\n",
    "            return decoder_outputs + encoder_outputs\n",
    "        \n",
    "        \n",
    "        # Update Final Decoder Layer  to Zeto\n",
    "        \n",
    "        decoder_outputs.last_hidden_state=decoder_outputs.last_hidden_state.data.fill_(0.00) \n",
    "\n",
    "        return Seq2SeqModelOutput(\n",
    "            last_hidden_state=decoder_outputs.last_hidden_state,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            decoder_attentions=decoder_outputs.attentions,\n",
    "            cross_attentions=decoder_outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "            encoder_attentions=encoder_outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70bb3c3",
   "metadata": {},
   "source": [
    "### Class for Initialization Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ffd5b801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/flan-t5-small were not used when initializing T5Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing T5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "T5Model = T5Model_Ov()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1550d29",
   "metadata": {},
   "source": [
    "### Rerun the Question answering after intizing  T5_Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a9baaa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "un est un\n"
     ]
    }
   ],
   "source": [
    "#tokenizer\n",
    "input_text=\"question-answering: What is sum of one and one ?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "enc_outputs = T5Model.encoder(input_ids)\n",
    "dec_outputs = T5Model.decoder(torch.tensor(enc_outputs.last_hidden_state.to(torch.int64)[0]))\n",
    "print(tokenizer.decode(enc_outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57233108",
   "metadata": {},
   "source": [
    "### Loan Question Answering Train File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57f9ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "with open('topiocqa/topiocqa_train.json') as f:\n",
    "    topiocqa_train_q_a = json.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8814cdeb",
   "metadata": {},
   "source": [
    "### <font size=10>Fine Tunning Flan-T5-small for Question Answering Task</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21390e75",
   "metadata": {},
   "source": [
    "### Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3bb40d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_model=AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\", return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13480e18",
   "metadata": {},
   "source": [
    "### Parameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c844b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = Adam(t5_model.parameters(), lr=0.00001)\n",
    "Q_LEN = 256   # Question Length\n",
    "T_LEN = 32    # Target Length\n",
    "BATCH_SIZE = 4\n",
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c17412f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    q_c_a=[]\n",
    "    for qca in data:\n",
    "        inputs={\"context\":str(qca['Context']),\"question\":str(qca[\"Question\"]),\"answer\":str(qca['Answer'])}\n",
    "        q_c_a.append(inputs)\n",
    "    return q_c_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21b23c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topiocqa_train_q_a_ctx=prepare_data(topiocqa_train_q_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a25957a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "topiocqa_train_q_a_ctx_df = pd.DataFrame(topiocqa_train_q_a_ctx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ecbeec28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>what was australia's contribution to the battl...</td>\n",
       "      <td>The army personnel and thousands of Australian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[what was australia's contribution to the batt...</td>\n",
       "      <td>was the battle fought in australia?</td>\n",
       "      <td>UNANSWERABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[what was australia's contribution to the batt...</td>\n",
       "      <td>when was the battle fought?</td>\n",
       "      <td>1944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[what was australia's contribution to the batt...</td>\n",
       "      <td>who fought in this battle?</td>\n",
       "      <td>Australians and British</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[what was australia's contribution to the batt...</td>\n",
       "      <td>was this battle part of a bigger war?</td>\n",
       "      <td>UNANSWERABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45445</th>\n",
       "      <td>[when does chloe find out about clarks powers,...</td>\n",
       "      <td>with regards to the actress who played sulliva...</td>\n",
       "      <td>Her first major television role was an episode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45446</th>\n",
       "      <td>[when does chloe find out about clarks powers,...</td>\n",
       "      <td>was she prosecuted on a criminal charge?</td>\n",
       "      <td>She was arrested in Brooklyn by the FBI on Apr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45447</th>\n",
       "      <td>[when does chloe find out about clarks powers,...</td>\n",
       "      <td>where were the scenes filmed during the making...</td>\n",
       "      <td>BB Studios in Burnaby, British Columbia.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45448</th>\n",
       "      <td>[when does chloe find out about clarks powers,...</td>\n",
       "      <td>who composed that television series' music score?</td>\n",
       "      <td>Mark Snow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45449</th>\n",
       "      <td>[when does chloe find out about clarks powers,...</td>\n",
       "      <td>did the series win any recognition?</td>\n",
       "      <td>Yes, several including Emmy for Outstanding So...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45450 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 context  \\\n",
       "0                                                     []   \n",
       "1      [what was australia's contribution to the batt...   \n",
       "2      [what was australia's contribution to the batt...   \n",
       "3      [what was australia's contribution to the batt...   \n",
       "4      [what was australia's contribution to the batt...   \n",
       "...                                                  ...   \n",
       "45445  [when does chloe find out about clarks powers,...   \n",
       "45446  [when does chloe find out about clarks powers,...   \n",
       "45447  [when does chloe find out about clarks powers,...   \n",
       "45448  [when does chloe find out about clarks powers,...   \n",
       "45449  [when does chloe find out about clarks powers,...   \n",
       "\n",
       "                                                question  \\\n",
       "0      what was australia's contribution to the battl...   \n",
       "1                    was the battle fought in australia?   \n",
       "2                            when was the battle fought?   \n",
       "3                             who fought in this battle?   \n",
       "4                  was this battle part of a bigger war?   \n",
       "...                                                  ...   \n",
       "45445  with regards to the actress who played sulliva...   \n",
       "45446           was she prosecuted on a criminal charge?   \n",
       "45447  where were the scenes filmed during the making...   \n",
       "45448  who composed that television series' music score?   \n",
       "45449                did the series win any recognition?   \n",
       "\n",
       "                                                  answer  \n",
       "0      The army personnel and thousands of Australian...  \n",
       "1                                           UNANSWERABLE  \n",
       "2                                                   1944  \n",
       "3                                Australians and British  \n",
       "4                                           UNANSWERABLE  \n",
       "...                                                  ...  \n",
       "45445  Her first major television role was an episode...  \n",
       "45446  She was arrested in Brooklyn by the FBI on Apr...  \n",
       "45447           BB Studios in Burnaby, British Columbia.  \n",
       "45448                                          Mark Snow  \n",
       "45449  Yes, several including Emmy for Outstanding So...  \n",
       "\n",
       "[45450 rows x 3 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topiocqa_train_q_a_ctx_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8952ba51",
   "metadata": {},
   "source": [
    "### Preprocess the Data and do Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ecd0f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QA_Dataset(Dataset):\n",
    "    def __init__(self, tokenizer, dataframe, q_len, t_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.q_len = q_len\n",
    "        self.t_len = t_len\n",
    "        self.data = dataframe\n",
    "        self.questions = self.data[\"question\"]\n",
    "        self.context = self.data[\"context\"]\n",
    "        self.answer = self.data['answer']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        context = self.context[idx]\n",
    "        answer = self.answer[idx]\n",
    "        \n",
    "        question_tokenized = self.tokenizer(question, context, max_length=self.q_len, padding=\"max_length\",\n",
    "                                                    truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
    "        answer_tokenized = self.tokenizer(answer, max_length=self.t_len, padding=\"max_length\", \n",
    "                                          truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
    "        \n",
    "        labels = torch.tensor(answer_tokenized[\"input_ids\"], dtype=torch.long)\n",
    "        labels[labels == 0] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(question_tokenized[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(question_tokenized[\"attention_mask\"], dtype=torch.long),\n",
    "            \"labels\": labels,\n",
    "            \"decoder_attention_mask\": torch.tensor(answer_tokenized[\"attention_mask\"], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c90628d",
   "metadata": {},
   "source": [
    "### Train Test Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6ed628ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(topiocqa_train_q_a_ctx_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_sampler = RandomSampler(train_data.index)\n",
    "val_sampler = RandomSampler(val_data.index)\n",
    "\n",
    "qa_dataset = QA_Dataset(tokenizer, topiocqa_train_q_a_ctx_df, Q_LEN, T_LEN)\n",
    "\n",
    "train_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "val_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14594a7",
   "metadata": {},
   "source": [
    "### Training for 5 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "505a2e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_model=t5_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c911deba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|█████████████████████████████████████████████████████████████| 9090/9090 [16:12<00:00,  9.35it/s]\n",
      "Validation batches: 100%|███████████████████████████████████████████████████████████| 2273/2273 [03:50<00:00,  9.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2 -> Train loss: 4.064034067404152\tValidation loss: 3.7232743387855836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|█████████████████████████████████████████████████████████████| 9090/9090 [16:41<00:00,  9.08it/s]\n",
      "Validation batches: 100%|███████████████████████████████████████████████████████████| 2273/2273 [03:55<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 -> Train loss: 4.065573071954798\tValidation loss: 3.7244674818818915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches:   4%|██▋                                                           | 392/9090 [00:41<15:14,  9.51it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training batches:  98%|████████████████████████████████████████████████████████████ | 8948/9090 [15:47<00:15,  9.42it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training batches:  71%|███████████████████████████████████████████▏                 | 6427/9090 [11:21<04:41,  9.46it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training batches:  46%|███████████████████████████▉                                 | 4162/9090 [07:20<08:38,  9.51it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Validation batches: 100%|███████████████████████████████████████████████████████████| 2273/2273 [03:50<00:00,  9.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/2 -> Train loss: 4.065605407954574\tValidation loss: 3.726745702503958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss = 0\n",
    "val_loss = 0\n",
    "train_batch_count = 0\n",
    "val_batch_count = 0\n",
    "\n",
    "for epoch in range(5):\n",
    "    t5_model.train()\n",
    "    for batch in tqdm(train_loader, desc=\"Training batches\"):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n",
    "\n",
    "        outputs = t5_model(\n",
    "                          input_ids=input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          labels=labels,\n",
    "                          decoder_attention_mask=decoder_attention_mask\n",
    "                        )\n",
    "\n",
    "        OPTIMIZER.zero_grad()\n",
    "        outputs.loss.backward()\n",
    "        OPTIMIZER.step()\n",
    "        train_loss += outputs.loss.item()\n",
    "        train_batch_count += 1\n",
    "    \n",
    "    #Evaluation\n",
    "    t5_model.eval()\n",
    "    for batch in tqdm(val_loader, desc=\"Validation batches\"):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n",
    "\n",
    "        outputs = t5_model(\n",
    "                          input_ids=input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          labels=labels,\n",
    "                          decoder_attention_mask=decoder_attention_mask\n",
    "                        )\n",
    "\n",
    "        OPTIMIZER.zero_grad()\n",
    "        outputs.loss.backward()\n",
    "        OPTIMIZER.step()\n",
    "        val_loss += outputs.loss.item()\n",
    "        val_batch_count += 1\n",
    "        \n",
    "    print(f\"{epoch+1}/{2} -> Train loss: {train_loss / train_batch_count}\\tValidation loss: {val_loss/val_batch_count}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0119bf8",
   "metadata": {},
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "26cd4052",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_model.save_pretrained(\"q&a_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0eb79cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f661c524160>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d62a84c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'name a few of the planets in the outer region?'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data['question']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d57dad",
   "metadata": {},
   "source": [
    "#### Check for Repsonce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "93119456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> osrio</s>\n"
     ]
    }
   ],
   "source": [
    "input_text='name a few of the planets in the outer region?'\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = t5_model.generate(input_ids.to(DEVICE))\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261c5297",
   "metadata": {},
   "source": [
    "###  Generate Prediction from Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9b41f8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 9090/9090 [14:51<00:00, 10.19it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions=[]\n",
    "for q_iter in tqdm(val_data['question']):\n",
    "    input_ids = tokenizer(q_iter, return_tensors=\"pt\").input_ids\n",
    "    outputs = t5_model.generate(input_ids.to(DEVICE))\n",
    "    predictions.append(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d52b01",
   "metadata": {},
   "source": [
    "### Get the Metrices out from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ae256747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.044862706740465254, 'rouge2': 0.0049390792052906585, 'rougeL': 0.04280564255114722, 'rougeLsum': 0.04281252560221893}\n",
      "{'bleu': 0.0013019711131864536, 'precisions': [0.04047318327394109, 0.004334933279722565, 0.0005248524738992284, 0.0001139953750447839], 'brevity_penalty': 0.7233238467630417, 'length_ratio': 0.7553450655036713, 'translation_length': 88676, 'reference_length': 117398}\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "#predictions=result\n",
    "rouge_score=rouge.compute(predictions=predictions,references=val_data['answer'].to_list())\n",
    "print(rouge_score)\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "bleu_score = bleu.compute(predictions=predictions, references=val_data['answer'].to_list())\n",
    "print(bleu_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
